Main()

	boolean success = job.waitForCompletion(true);

Job.java

	waitForCompletion

	if (state == JobState.DEFINE) {
      submit();
    }

    public void submit() throws IOException, InterruptedException, 
                              ClassNotFoundException {
    ensureState(JobState.DEFINE);
    setUseNewAPI();
    
    // Connect to the JobTracker and submit the job
    connect();
    info = jobClient.submitJobInternal(conf);
    super.setJobID(info.getID());
    state = JobState.RUNNING;
   }

JobClient.java

	

	this.rpcJobSubmitClient = 
          createRPCProxy(JobTracker.getAddress(conf), conf);
    this.jobSubmitClient = createProxy(this.rpcJobSubmitClient, conf);


    status = jobSubmitClient.submitJob(
              jobId, submitJobDir.toString(), jobCopy.getCredentials());

JobTracker.java
	
	submitJob()

		status = addJob(jobId, job);

	(details from this step to next is in this site http://www.jiancool.com/article/8165570161/;jsessionid=DA095379D6634B03536D6B2FD3286707)

	public void initJob(JobInProgress job)

	job.initTasks();

JobinProgress.java->
	
	TaskSplitMetaInfo[] splits = createSplits(jobId);

		TaskSplitMetaInfo[] allTaskSplitMetaInfo =
	      SplitMetaInfoReader.readSplitMetaInfo(jobId, fs, jobtracker.getConf(),
	          jobSubmitDir);
		SplitMetaInfoReader
			mapreduce.jobtracker.split.metainfo.maxsize

			read in SplitMetaInfo[]
			construct TaskSplitIndex (split file path and start offset of a split)
			construct TaskSplitMetaInfo (TaskSplitIndex, splits locations, split length)

	maps[i] = new TaskInProgress(jobId, jobFile, 
                                   splits[i], 
                                   jobtracker, conf, this, i, numSlotsPerMap);

TaskInProgress.java

	t = new MapTask(jobFile, taskid, partition, splitInfo.getSplitIndex(),
                      numSlotsNeeded);
MapTask.java

	runNewMapper(job, splitMetaInfo, umbilical, reporter);

	org.apache.hadoop.mapreduce.InputSplit split = null;
    split = getSplitDetails(new Path(splitIndex.getSplitLocation()),
        splitIndex.getStartOffset());

********************************************************************************************************

SampleSplit  serialization

SampleSplit is written to file

most important SampleSplit meta including: 

1. split serialization file path: JobSubmissionFiles.getJobSplitFile(jobSubmitDir).toString()

2. offset of a split in serialization file: recorded by SplitWriter

3. splits locations: interface of InputSplit

4. split length: interface of InputSplit


/***********************************************************
directly generate samples for subset of whole data without pre-filtering out subset.

workload


how traditional mr do computations: draw digram


sendvalue for all weights

****************************************tiny image data set****
% valid field names:
% 1. keyword	0
% 2. filename	80
% 3. width		175
% 4. height		177
% 5. colors		179
% 6. date		183
% 7. engine 	212
% 8. thumb_url	222
% 9. source_url 422
% 10. page 		750
% 11. ind_page  754
% 12. ind_engine 758
% 13. ind_overall	762
% 14. label (1 = correct, 0 = incorrect, -1 = unlabelled) 766

******************************************************************************
efficient online

"mapreduce.input.fileinputformat.split.minsize.per.node"
"mapreduce.input.fileinputformat.split.minsize.per.rack"
"mapreduce.input.fileinputformat.split.maxsize"


clusterID = ((NewTrackingRecordReader)reader).getRecordReader();

ssh -N -f -L 54444:compute-0-0:8080 xzhang@cass.eecs.ucf.edu


hadoop jar hadoop-SubsetApprox-1.2.1.jar org.apache.hadoop.mapreduce.approx.app.GitHubEvent -s 100000 -w 0=PushEvent -t github -i /2015-01.json -o /github

hadoop jar hadoop-SubsetApprox-1.2.1.jar org.apache.hadoop.mapreduce.approx.index.IndexUserVisit -s 10000 -f 2-4-5 -t uservisit -i /test/uservist.test

hadoop jar hadoop-SubsetApprox-1.2.1.jar org.apache.hadoop.mapreduce.approx.index.IndexGitHub -s 10000 -f 1 -t githuborg -i /2015-01.json

github


def printType1(file):
	a = json.loads(file.readline())
	print(a['created_at'])
	return a

def printType3(file):
	a = json.loads(file.readline())
	print(a['type'])
	if str(a['type']) =="IssueCommentEvent":
		print(a['payload']['issue']['created_at'])
	return a


push event
['payload']['size']
['org'] true or false
issuecomments event

['payload']['issue']['comments']

pullrequest

c['payload']['pull_request']['additions']

fork
['payload']['forkee']['size']
[][]['watchers'] watchers_count

issue event

['payload']['issue']['comments']

33GB, 11:46:43----11:49:55

precise 228s
15/09/30 21:15:09 INFO mapred.JobClient:     Map output records=6841398
2015-09-30 21:11:21


approximate 20s
15/09/30 21:50:49 INFO input.FileInputFormat: Total input paths to process : 1
15/09/30 21:51:09 INFO mapred.JobClient:     Map output records=154497



************************
small data
**********************

github

repository id (storage cost experiment)
issuecomments  issue date
type
org

1000, 10000, 100000

amazon
reviewTime
categories
brand



payload.issue.created_at=2015-01-0,type=IssueCommentEvent